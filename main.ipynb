{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e6dd44-75c2-4f2e-856b-b41df896018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d1f689-d7b2-429e-831a-0f1030719bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394083f-ed92-46d0-b224-5dfa98dd9be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2341b9a3-d2d1-4959-a7d3-9e802cb4fb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.9 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.1/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 2.4/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 2.9/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 2.9/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.9/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.5/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.7/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 6.0/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.3/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.0/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Total Sentences Extracted: 891\n",
      "Sample Sentences: ['How to turn your expertise into \\nmagnetic marketing material\\n\\t\\nby\\tJonathan Kranz\\n designed by\\tCiano Design\\n Copyright © 2009 Jonathan Kranz \\nThis work is licensed under the Creative Commons \\nAttribution-Noncommercial-No Derivative Works \\n3.0 United States License.', 'To view a copy of this \\nlicense, visit http://creativecommons.org/licenses/\\nby-nc-nd/3.0/us/ or send a letter to Creative \\nCommons, 171 Second Street, Suite 300, \\nSan Francisco, California, 94105, USA.\\n Introduction. .  ', '.  .  ', '.  .  ', '.  .  ']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def load_pdf(filepath):\n",
    "    \"\"\"Extract text from a PDF and tokenize sentences using spaCy.\"\"\"\n",
    "    doc = fitz.open(filepath)\n",
    "    text = \" \".join([page.get_text(\"text\") for page in doc])\n",
    "    \n",
    "    # Use spaCy for sentence tokenization\n",
    "    return [sent.text for sent in nlp(text).sents]\n",
    "\n",
    "book_sentences = load_pdf(\"ebookebook.pdf\")\n",
    "print(f\"Total Sentences Extracted: {len(book_sentences)}\")\n",
    "print(\"Sample Sentences:\", book_sentences[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8aaab3-bf68-4d44-bdb2-2211daaab4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model\n",
    "\n",
    "# Convert book sentences into embeddings\n",
    "sentence_embeddings = model.encode(book_sentences, convert_to_numpy=True)\n",
    "\n",
    "# Create FAISS Index for fast retrieval\n",
    "index = faiss.IndexFlatL2(sentence_embeddings.shape[1])\n",
    "index.add(sentence_embeddings)\n",
    "\n",
    "# Save the index and data for later use\n",
    "faiss.write_index(index, \"faiss_book_index.bin\")\n",
    "np.save(\"book_sentences.npy\", book_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc64a00-55e0-4dbf-8149-4ade5fc5e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context: ['Let the book’s ideas \\nand intended readership guide your thematic choices.\\n', 'What’s in it for the reader?\\n', 'I’ll explain the introduction and conclusion later in the book.']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_text(question, top_k=3):\n",
    "    question_embedding = model.encode([question], convert_to_numpy=True)\n",
    "    _, indices = index.search(question_embedding, top_k)\n",
    "    return [book_sentences[i] for i in indices[0]]\n",
    "\n",
    "question = \"What is the main theme of the book?\"\n",
    "retrieved_text = retrieve_relevant_text(question)\n",
    "print(\"Retrieved Context:\", retrieved_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b938f573-e691-4a2f-aed4-72b598208278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.48.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (2.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf31ce5-677c-4ae0-82e3-85022c13d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and sentence embeddings saved!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a lightweight sentence embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert book sentences into vector embeddings\n",
    "sentence_embeddings = embedding_model.encode(book_sentences, convert_to_numpy=True)\n",
    "\n",
    "# Create a FAISS index for fast similarity search\n",
    "index = faiss.IndexFlatL2(sentence_embeddings.shape[1])\n",
    "index.add(sentence_embeddings)\n",
    "\n",
    "# Save the FAISS index and sentences for later use\n",
    "faiss.write_index(index, \"faiss_book_index.bin\")\n",
    "np.save(\"book_sentences.npy\", np.array(book_sentences))\n",
    "\n",
    "print(\"FAISS index and sentence embeddings saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e240546-ef48-48dd-9a5b-59775979b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context: ['Let the book’s ideas \\nand intended readership guide your thematic choices.\\n', 'What’s in it for the reader?\\n', 'I’ll explain the introduction and conclusion later in the book.']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_text(question, top_k=3):\n",
    "    \"\"\"Retrieve the most relevant sentences from the book for a given question.\"\"\"\n",
    "    question_embedding = embedding_model.encode([question], convert_to_numpy=True)\n",
    "    _, indices = index.search(question_embedding, top_k)  # Retrieve top K closest matches\n",
    "    return [book_sentences[i] for i in indices[0]]\n",
    "\n",
    "# Test retrieval system\n",
    "sample_question = \"What is the main theme of the book?\"\n",
    "retrieved_text = retrieve_relevant_text(sample_question)\n",
    "print(\"Retrieved Context:\", retrieved_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "994ae141-cc86-475d-bbb8-0d752f02ef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7e5630f-5b26-4030-b316-7086723bc4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.48.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a18bb5-81b0-4066-8d93-749f6926e98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dad485c962b4855b6363d6da9090823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Parth\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e680514485e0499dadb822d837964ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e934b9d0d1489684486deca7397f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "qa_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "print(\"T5 Tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ebb21a-8a00-4485-9d6a-6bd5ac79ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context: [np.str_('Let the book’s ideas \\nand intended readership guide your thematic choices.\\n'), np.str_('What’s in it for the reader?\\n'), np.str_('I’ll explain the introduction and conclusion later in the book.')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the saved FAISS index and sentences\n",
    "index = faiss.read_index(\"faiss_book_index.bin\")\n",
    "book_sentences = np.load(\"book_sentences.npy\", allow_pickle=True)\n",
    "\n",
    "def retrieve_relevant_text(question, top_k=3):\n",
    "    \"\"\"Retrieve the most relevant sentences from the book for a given question.\"\"\"\n",
    "    question_embedding = embedding_model.encode([question], convert_to_numpy=True)\n",
    "    _, indices = index.search(question_embedding, top_k)  # Find closest sentences\n",
    "    return [book_sentences[i] for i in indices[0]]\n",
    "\n",
    "# Example test\n",
    "sample_question = \"What is the main theme of the book?\"\n",
    "retrieved_text = retrieve_relevant_text(sample_question)\n",
    "print(\"Retrieved Context:\", retrieved_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed98ea5e-c1ad-4ffa-bf26-f04ae89b147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the main theme of the book?\n",
      "A: readership\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "qa_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    \"\"\"Generate an answer using the T5 model based on retrieved context.\"\"\"\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = qa_tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    output_ids = qa_model.generate(input_ids, max_length=50)\n",
    "    return qa_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the QA model with a sample question\n",
    "retrieved_context = \" \".join(retrieve_relevant_text(sample_question))\n",
    "answer = generate_answer(sample_question, retrieved_context)\n",
    "print(f\"Q: {sample_question}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b504dd7-9ad8-4182-b333-3de5e566a789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who is the main character?\n",
      "A: St. Jacques\n",
      "\n",
      "Q: What happens in the first chapter?\n",
      "A: offer reinforcing information of value\n",
      "\n",
      "Q: What is the moral of the story?\n",
      "A: not one effort\n",
      "\n",
      "Q: What did the author emphasize?\n",
      "A: They were able to use the book to start productive conversations.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Who is the main character?\",\n",
    "    \"What happens in the first chapter?\",\n",
    "    \"What is the moral of the story?\",\n",
    "    \"What did the author emphasize?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    context = \" \".join(retrieve_relevant_text(q))\n",
    "    answer = generate_answer(q, context)\n",
    "    print(f\"Q: {q}\\nA: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d05577-7b9c-4f53-852a-99f5a6a3e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "qa_model.save_pretrained(\"qa_model\")\n",
    "qa_tokenizer.save_pretrained(\"qa_model\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe064e0-7450-4ea9-9586-2e3e51e2d992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
